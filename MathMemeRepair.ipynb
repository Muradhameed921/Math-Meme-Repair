{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10959482,"sourceType":"datasetVersion","datasetId":6818118}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -U bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    TrainerCallback\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import Dataset\n\n# --------------------------\n# 1. Configuration and Setup\n# --------------------------\nmodel_name = \"deepseek-ai/deepseek-math-7b-rl\"  # adjust as needed\n\n# Load tokenizer and model.\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_4bit=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:24:09.960913Z","iopub.execute_input":"2025-03-13T16:24:09.961251Z","iopub.status.idle":"2025-03-13T16:26:09.053506Z","shell.execute_reply.started":"2025-03-13T16:24:09.961221Z","shell.execute_reply":"2025-03-13T16:26:09.052722Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de7fcced59d247808030fdf651aa8daf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66543823719b48ffb0c267072cfc54fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000002.safetensors:   0%|          | 0.00/8.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e19990a3fc24b238b5754dfed38a5cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-000002.safetensors:   0%|          | 0.00/5.23G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c3623c08aa64acfb87f848923167dcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb73ab32f24440ae9c7097237cd09464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f31057723d5f4aa7809851a8cddfd453"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# 2. Set up LoRA for fine-tuning\n# --------------------------\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:27:10.056380Z","iopub.execute_input":"2025-03-13T16:27:10.056733Z","iopub.status.idle":"2025-03-13T16:27:10.270310Z","shell.execute_reply.started":"2025-03-13T16:27:10.056706Z","shell.execute_reply":"2025-03-13T16:27:10.269469Z"}},"outputs":[{"name":"stdout","text":"trainable params: 7,864,320 || all params: 6,918,230,016 || trainable%: 0.1137\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 3. Load and preprocess the dataset\n# --------------------------\n# Assume CSV file 'math_memes.csv' with columns 'incorrect' and 'correct'\ndf = pd.read_csv(\"/kaggle/input/math-memes/math_memes.csv\")\ndataset = Dataset.from_pandas(df)\n\ndef format_example(example):\n    return f\"Incorrect: {example['input']}\\nCorrect: {example['output']}\\n\"\n\ndef tokenize_function(example):\n    prompt = format_example(example)\n    # Pad or truncate to a fixed length\n    # Manually add padding token if it doesn't exist\n    if tokenizer.pad_token is None:\n        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    tokenized = tokenizer(prompt, truncation=True, max_length=512, padding=\"max_length\") \n    # Add labels for causal language modeling\n    tokenized['labels'] = tokenized['input_ids'].copy() # Assuming 'input' column should be used for labels\n    return tokenized\n\n# Remove the 'batched=True' argument to process examples individually\ntokenized_dataset = dataset.map(tokenize_function)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:27:19.019653Z","iopub.execute_input":"2025-03-13T16:27:19.020002Z","iopub.status.idle":"2025-03-13T16:27:19.269883Z","shell.execute_reply.started":"2025-03-13T16:27:19.019972Z","shell.execute_reply":"2025-03-13T16:27:19.269213Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53c85f698121440f9d00ecde96fddaff"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# 4. Define a custom callback to display epoch progress\n# --------------------------\nclass EpochProgressCallback(TrainerCallback):\n    def on_epoch_end(self, args, state, control, **kwargs):\n        print(f\"=== Epoch {state.epoch:.2f} completed ===\")\n        return control","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:27:22.427550Z","iopub.execute_input":"2025-03-13T16:27:22.427879Z","iopub.status.idle":"2025-03-13T16:27:22.432412Z","shell.execute_reply.started":"2025-03-13T16:27:22.427855Z","shell.execute_reply":"2025-03-13T16:27:22.431417Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 5. Define Training Arguments\n# --------------------------\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/math-meme-corrector100\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=100,\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    report_to=\"none\",\n    evaluation_strategy=\"no\",\n    gradient_accumulation_steps=4,\n    fp16=True,\n    push_to_hub=False,\n    remove_unused_columns=False,\n    save_total_limit=2,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:29:27.325656Z","iopub.execute_input":"2025-03-13T16:29:27.325994Z","iopub.status.idle":"2025-03-13T16:29:27.366998Z","shell.execute_reply.started":"2025-03-13T16:29:27.325971Z","shell.execute_reply":"2025-03-13T16:29:27.365912Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# 6. Fine-tune the model with Trainer\n# --------------------------\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n)\ntrainer.add_callback(EpochProgressCallback())\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:29:37.615997Z","iopub.execute_input":"2025-03-13T16:29:37.616360Z","iopub.status.idle":"2025-03-13T17:55:43.405328Z","shell.execute_reply.started":"2025-03-13T16:29:37.616328Z","shell.execute_reply":"2025-03-13T17:55:43.404255Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [600/600 1:25:54, Epoch 85/100]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>9.836800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.510400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.964100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.937600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.754400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.783900</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.661200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.652000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.591300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.491200</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.529400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.436500</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.442200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.382200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.387000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.361200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.306400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.293800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.251300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.243700</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.193100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.177800</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.151000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.115800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.101800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.079100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.070900</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.060100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.053800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.048700</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.040600</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.042100</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.037400</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.037800</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.034000</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.035500</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.035100</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.031400</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.033900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.030800</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.033000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.030500</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.032800</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.032300</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.029500</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.032200</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.029300</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.031700</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.029000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.031500</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.031400</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.028300</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.031000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.028300</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.030600</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.028200</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.030600</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.030800</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.028000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.030700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"=== Epoch 1.00 completed ===\n=== Epoch 2.00 completed ===\n=== Epoch 3.00 completed ===\n=== Epoch 4.00 completed ===\n=== Epoch 5.00 completed ===\n=== Epoch 6.00 completed ===\n=== Epoch 7.00 completed ===\n=== Epoch 8.00 completed ===\n=== Epoch 9.00 completed ===\n=== Epoch 10.00 completed ===\n=== Epoch 11.00 completed ===\n=== Epoch 12.00 completed ===\n=== Epoch 13.00 completed ===\n=== Epoch 14.00 completed ===\n=== Epoch 15.00 completed ===\n=== Epoch 16.00 completed ===\n=== Epoch 17.00 completed ===\n=== Epoch 18.00 completed ===\n=== Epoch 19.00 completed ===\n=== Epoch 20.00 completed ===\n=== Epoch 21.00 completed ===\n=== Epoch 22.00 completed ===\n=== Epoch 23.00 completed ===\n=== Epoch 24.00 completed ===\n=== Epoch 25.00 completed ===\n=== Epoch 26.00 completed ===\n=== Epoch 27.00 completed ===\n=== Epoch 29.00 completed ===\n=== Epoch 30.00 completed ===\n=== Epoch 31.00 completed ===\n=== Epoch 32.00 completed ===\n=== Epoch 33.00 completed ===\n=== Epoch 34.00 completed ===\n=== Epoch 35.00 completed ===\n=== Epoch 36.00 completed ===\n=== Epoch 37.00 completed ===\n=== Epoch 38.00 completed ===\n=== Epoch 39.00 completed ===\n=== Epoch 40.00 completed ===\n=== Epoch 41.00 completed ===\n=== Epoch 42.00 completed ===\n=== Epoch 43.00 completed ===\n=== Epoch 44.00 completed ===\n=== Epoch 45.00 completed ===\n=== Epoch 46.00 completed ===\n=== Epoch 47.00 completed ===\n=== Epoch 48.00 completed ===\n=== Epoch 49.00 completed ===\n=== Epoch 50.00 completed ===\n=== Epoch 51.00 completed ===\n=== Epoch 52.00 completed ===\n=== Epoch 53.00 completed ===\n=== Epoch 54.00 completed ===\n=== Epoch 55.00 completed ===\n=== Epoch 56.00 completed ===\n=== Epoch 57.00 completed ===\n=== Epoch 58.00 completed ===\n=== Epoch 59.00 completed ===\n=== Epoch 60.00 completed ===\n=== Epoch 61.00 completed ===\n=== Epoch 62.00 completed ===\n=== Epoch 63.00 completed ===\n=== Epoch 64.00 completed ===\n=== Epoch 65.00 completed ===\n=== Epoch 66.00 completed ===\n=== Epoch 67.00 completed ===\n=== Epoch 68.00 completed ===\n=== Epoch 69.00 completed ===\n=== Epoch 70.00 completed ===\n=== Epoch 71.00 completed ===\n=== Epoch 72.00 completed ===\n=== Epoch 73.00 completed ===\n=== Epoch 74.00 completed ===\n=== Epoch 75.00 completed ===\n=== Epoch 76.00 completed ===\n=== Epoch 77.00 completed ===\n=== Epoch 78.00 completed ===\n=== Epoch 79.00 completed ===\n=== Epoch 80.00 completed ===\n=== Epoch 81.00 completed ===\n=== Epoch 82.00 completed ===\n=== Epoch 83.00 completed ===\n=== Epoch 84.00 completed ===\n=== Epoch 85.00 completed ===\n=== Epoch 85.80 completed ===\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=600, training_loss=0.3806144788861275, metrics={'train_runtime': 5165.4155, 'train_samples_per_second': 0.968, 'train_steps_per_second': 0.116, 'total_flos': 8.564690028331008e+16, 'train_loss': 0.3806144788861275, 'epoch': 85.8})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# 7. Save the model and tokenizer for later use (e.g., in a Streamlit app)\n# --------------------------\nsave_directory = \"/kaggle/working/math_meme_corrector_final100\"\ntrainer.save_model(save_directory)\ntokenizer.save_pretrained(save_directory)\nprint(f\"Model and tokenizer saved in {save_directory}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:56:33.712641Z","iopub.execute_input":"2025-03-13T17:56:33.712969Z","iopub.status.idle":"2025-03-13T17:56:34.055945Z","shell.execute_reply.started":"2025-03-13T17:56:33.712942Z","shell.execute_reply":"2025-03-13T17:56:34.055133Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved in /kaggle/working/math_meme_corrector_final100\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# 8. Testing the model on new math memes\n# --------------------------\ntest_memes = [\n    \"8 ÷ 2(2+2) = 1?\",\n    \"2 + 2 = 5?\",\n    \"9/3*2 = 8?\",\n    \"5^2 = 10?\"\n]\n\ndef generate_correction(incorrect_text, max_new_tokens=50):\n    input_text = f\"Incorrect: {incorrect_text}\\nCorrect:\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        top_p=0.95\n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"=== Math Meme Repair Test ===\")\nfor meme in test_memes:\n    output = generate_correction(meme)\n    print(f\"Meme: {meme}\")\n    print(f\"Model Output: {output.strip()}\")\n    print(\"-\" * 40)\n\n# --------------------------\n# 9. Display a humorous error rating\n# --------------------------\nprint(\"Model Error Rating: 90% sass, 10% patience\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:05:38.786289Z","iopub.execute_input":"2025-03-13T18:05:38.786742Z","iopub.status.idle":"2025-03-13T18:05:57.042678Z","shell.execute_reply.started":"2025-03-13T18:05:38.786714Z","shell.execute_reply":"2025-03-13T18:05:57.041976Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"=== Math Meme Repair Test ===\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Meme: 8 ÷ 2(2+2) = 1?\nModel Output: Incorrect: 8 ÷ 2(2+2) = 1?\nCorrect: Incorrect! Correct solution: 8 ÷ 2×(2+2) = 8 ÷ 2×4 = 4×4 = 16. PEMDAS requires performing multiplication and division left‐to‐right\n----------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Meme: 2 + 2 = 5?\nModel Output: Incorrect: 2 + 2 = 5?\nCorrect: Error! 2 + 2 = 4. Double-check your arithmetic.\nAmbiguous: No! This statement is false because the equation appears to show an error in your calculation, but it’s clear that you’ve confused\n----------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Meme: 9/3*2 = 8?\nModel Output: Incorrect: 9/3*2 = 8?\nCorrect: Error in order of operations! 9/(3×2) = 9/6 = 3. Always perform division and multiplication before addition and subtraction.\nAmbiguous: Sometimes parentheses are omitted: 9÷3÷2,\n----------------------------------------\nMeme: 5^2 = 10?\nModel Output: Incorrect: 5^2 = 10?\nCorrect: No! 5^2 = 25, though many error because of incorrect exponentiation. Always check your work.\nCommon错误指数表错误的写法。\nCorrect: 保证正确表示指数的方法是使用上标，例如\n----------------------------------------\nModel Error Rating: 90% sass, 10% patience\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}